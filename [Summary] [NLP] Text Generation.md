### [Long Short term memory](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.676.4320&rep=rep1&type=pdf), 1997

### [A neural probablistic language model](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf), 2003

**Problem Statement:** Learning a distributed representation for words and probability function for word sequences

**Dataset:** Brown corpus: 1.1m words, Assoated Press: 14 mil words

**Experiment:**

1. Associate each word in the vocab with distributed word feature vector

2. Express joint probability function of word sequences in terms of features vectors

3. Learn simultaneously the word feature vectors and parameters of probability function

**Models:** 
n-gram models and modified kneser ney algorithms	

**Conclusion:** text set perplexity, and convergence of stochastic gradient ascent procedure

### [Generating Sequences with Recurrent Neural Networks](https://arxiv.org/pdf/1308.0850.pdf), 2013

**Probelm Statement**: Train a recurrent neural network to maximize the log-likelihood of each ground-truth word given prior observed words

**Limitations:** Exposure bias: discrepancy between training and inference stage

**Other Notes:**
A main drawback of existing methods to long text generation is that the binary guiding signal from D is sparse as it is only available when the whole text sample is generated. Also, the scalar guiding signal for a whole text is non-informative as it does not necessarily preserve the pic- ture about the intermediate syntactic structure and semantics of the text that is being generated for G to sufficiently learn.

### [Scheduled Sampling for sequence prediction](https://papers.nips.cc/paper/5956-scheduled-sampling-for-sequence-prediction-with-recurrent-neural-networks.pdf), 2015

**Problem Statement:** During inference, true previous target tokens are unavailable, and have to rely on tokens generated by the model. Mistakes made early are amplified.	MSCOCO image captioning challenge 2015; constituency parsing; speech recognition

**Dataset:**
- Image Captioning: Training procedure picks a caption at random for mini-batch of examples, and optimise objective function. Used scheduled sampling method
- Constituency Parsing: Input word dictionary with 90k words, target dictionary with 128 words to describe the tree 
- Speech Recognition: Training example is input/output pairs

**Experiment:**
Curriculum learning approach to force the model to deal with its own mistakes. Slowly changing the training objective where the previous token is known (easy task) to previous token is generated (hard task)

### [Adverserial Feature Matching for Text Generation (TextGAN)](https://arxiv.org/pdf/1706.03850.pdf), 2017

**Problem Statement:** Generate realistic synthetic data using high dimensional latent feature distributions of real and synthetic sentences via a kernelised discrepancy metric

**Dataset:** 
- BookCorpus Dataset (Zhu et al, 2016), 70 million sentences
- ArXiv dataset: 5 million sentences
- Merging both corpora to generate sentences that integrate scientific and informal writing styles

**Experiment:**
- Train generator/ discriminator iteratively, using 5 isotropic Gaussian RBF kernels with different bandwidths, selected to be close to the median distance of feature vectors encoded from real sentences 

**Models:**
- LSTM as generator, CNN as discriminator
- Kernel-based Moment-matching scheme introducing Kernal Hilbert Space, to force empirical distributions of real and synthetic sentences to have matched moments in latent-feature space. TO encourage model to learn representations that are both informative of the original sentences and distriminative wrt synthetic sentences 

**Limitations:**
- Long range distance features not difficult to abstract by discriminator, less likely to be imitated by generator 
- Future work: disentangle latent representations for different writing styles

**Code Online:** https://github.com/williamSYSU/TextGAN-PyTorch

**Experiments:**
- Text generation formalised as a sequential decision making process

**Model used:**
- GAN settings: CNN + LSTM
- Evaluation: Negative log likelihood + BLEU statistics + human evaluation scores

**Conclusions:**
LeakGAN	Highly effective in long text generation, improves performance in short text generation	discriminator reveals its internal state when guding the generator - adverserial AI use case?

### [Long text generation with adverserial training and leaked information](https://arxiv.org/pdf/1709.08624.pdf), 2017

**Problem Statement:** Propose new algorithmic framework to address non-informativeness and sparsity issues

**Dataset:** EMNLP2017 WMT, COCO Image Cpations, Chinese Poems



### [Neural Text Generation: A practical Guide](https://arxiv.org/pdf/1711.09534.pdf), 2017

### [MaskGAN: Better Text Generation by filling in the ____-](https://arxiv.org/pdf/1801.07736.pdf), 2018

**Problem Statement:** Portions of a body of text are deleted, to infill while indistinguishable from original set

**Experiment:** 
1. Train a language model using MLE. Use pretrained model weights for seq2seq encoder/decoder modules. 
2. Pretrain seq2seq models on in-filling task using MLE, using attention parameters. 
3. Select model producing lowest validation perplexity. 

**Result**: Training where contiguous blocks of words were masked produced better samples. Use of attention was important for in-filled words to be sufficiently conditioned on the input context

**Code:** https://github.com/tensorflow/models/tree/ master/research/maskgan

### [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf),	2017	

**Problem Statement:**
Proposes the use of a transformer architecture, based solely on attention sans recurrent mechanisms. Achieves S.O.A. performance with a tenth to a hundreth of previous algorithms' cost	

**Dataset:**
WMT 2014 (EN-DE & EN-FE)	

**Experiments:**
- To evaluate the new architecture, used ablation studies on model hyperparemeters. 
- regularization using dropout (0.1)
- compared speed and accuracy separately against prev SOA(bytenet, MoE, ConvS2S)
- only translation task is evaluated"	the Transformer ()

**Conclusion:**
Claimed to be fastest and most accurate so far	none known currently	

**Code online:**
https://github.com/tensorflow/models/tree/master/official/transformer

###[MaskGAN: Better Text Generation by filling in the ____](https://arxiv.org/pdf/1801.07736.pdf), 2018	P

**Problem Statement:**
Portions of a body of text are deleted, to infill while indistinguishable from original set	

**Experiment:**
Train a language model using MLE. Use pretrained model weights for seq2seq encoder/decoder modules. Pretrain seq2seq models on in-filling task using MLE, using attention parameters. Select model producing lowest validation perplexity. 	

**Model:**
seq2seq	

**Conclusion:**
Training where contiguous blocks of words were masked produced better samples. Use of attention was important for in-filled words to be sufficiently conditioned on the input context		

**Code Online:**
https://github.com/tensorflow/models/tree/ master/research/maskgan	

### [Neural text generation: past present beyond](https://arxiv.org/pdf/1803.07133.pdf), 2018

**Problem Statement:** Empircal study of typical neural text generation models with image captioning tasks: SeqGAN, MaliGAN, RankGAN, LeakGAN, MaskGAN, Text GAN, MLE

**Dataset:** Conditional and unconditional samples generated on Penn Treebank and IMDB datasets at word level

**Experiment:** Initialise GAN model parameters in a standard Gaussian distribution, pretrain each models' generator and discriminator using the MLE training of 80 epochs

**Models:** SeqGAN, MaliGAN, RankGAN, LeakGAN, MaskGAN, Text GAN, MLE

**Results:** Measure: BLEU score. SeqGAN works best, MaliGAN works best on long texts generation

### [Survey of the state of the art natural language generation: core tasks, applications and evaluation](https://arxiv.org/pdf/1703.09902.pdf), 2018

**Notes:** BLEU score is biased towards length of texts. With short texts, n-gram based measures do not perform well 

### [Recent Trends in Deep Learning Based Natural Language Processing](https://arxiv.org/pdf/1708.02709.pdf),	2018

### [Disentangled Reprsentation Learning for Non-Parallel Text Style Transfer](https://arxiv.org/pdf/1808.04339.pdf), 2018

**Problem Statement:**
Transfer accuracy, content preservation and language fluency: disentangle style and content to the task

**Dataset:**
- Yelp Reviews
- Amazon Reviews

**Experiment:**
- Disentangling Latent Space: analyse how style (sentiment) and content disentangled. Use DAEs and VAEs
- Non-Parallel Text Style Transfer

**Models:**
DAE, VAEs	

**Metrics: **
- Style Transfer accuracy by comparing sentiment
- Cosine similarity, but not sensitive measure
- Word overlap as effective measure counting unigram word overlap rate
- Language Fluency: Using trigram Kneser-Ney smoothed language model 
- Manual Evaluation	

**Code online**
https://github.com/ vineetjohn/linguistic-style-transfer

### [Lagging Inference Networks and Posterior Collapse in VAEs](https://arxiv.org/pdf/1901.05534.pdf), 2019

### [NAACL Tutorial on GANs](	https://sites.cs.ucsb.edu/~william/papers/AdvNLP-NAACL2019.pdf), 2019

### [Language models as knowledge bases?](https://arxiv.org/pdf/1909.01066.pdf), 2019

**Code:** https://github.com/facebookresearch/LAMA

### [Style Transfer in Text (Github link of papers)](https://github.com/fuzhenxin/Style-Transfer-in-Text)

###[Unsupervised Question Answering by Cloze Translation](https://arxiv.org/abs/1906.04980), 2019	

**Problem Statement**
Proposed a method of constructing a trainset using unsupervised methods, and using an established BERT + Self-attn model to train on it. Claim that the hardest part of QA is getting a labelled QA datast	unsup learning on wiki articles, and evluated on established QA dataset SQuAD 2.0	

**Experiment:**
- 3 phases. Close generation by extracting clauses and masking key named entity. Close translation by translating these masked clauses into natrual questions using seq to seq model in an unsupervised manner. Followed by training a QA model on it (BERT + SA)
- After training, evaluated using EM and F1 score on other unsup and other sup models. It is the best unsup, but is much inferior to sup models"	BERT + SA. used a lot of parses for Named entity recognition, spacy etc	Proposed unsup method of learning QA without labelled data (i.e. unsup)		

**Code online:**
https://github.com/facebookresearch/UnsupervisedQA	

**Remarks**
While this unsup pipeline might produce resonable performance for QA machine, a well labelled dataset of 300 examples can equal it. 
