### [Long Short term memory](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.676.4320&rep=rep1&type=pdf), 1997

### [A neural probablistic language model](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf), 2003

**Problem Statement:** Learning a distributed representation for words and probability function for word sequences

**Dataset:** Brown corpus: 1.1m words, Assoated Press: 14 mil words

**Experiment:**

1. Associate each word in the vocab with distributed word feature vector

2. Express joint probability function of word sequences in terms of features vectors

3. Learn simultaneously the word feature vectors and parameters of probability function

**Models:** 
n-gram models and modified kneser ney algorithms	

**Conclusion:** text set perplexity, and convergence of stochastic gradient ascent procedure

### [Scheduled Sampling for sequence prediction](https://papers.nips.cc/paper/5956-scheduled-sampling-for-sequence-prediction-with-recurrent-neural-networks.pdf), 2015

**Problem Statement:** During inference, true previous target tokens are unavailable, and have to rely on tokens generated by the model. Mistakes made early are amplified.	MSCOCO image captioning challenge 2015; constituency parsing; speech recognition

**Dataset:**
- Image Captioning: Training procedure picks a caption at random for mini-batch of examples, and optimise objective function. Used scheduled sampling method
- Constituency Parsing: Input word dictionary with 90k words, target dictionary with 128 words to describe the tree 
- Speech Recognition: Training example is input/output pairs

**Experiment:**
Curriculum learning approach to force the model to deal with its own mistakes. Slowly changing the training objective where the previous token is known (easy task) to previous token is generated (hard task)

### [Long text generation with adverserial training and leaked information](https://arxiv.org/pdf/1709.08624.pdf), 2017

### [Neural Text Generation: A practical Guide](https://arxiv.org/pdf/1711.09534.pdf), 2017

### [MaskGAN: Better Text Generation by filling in the ____-](https://arxiv.org/pdf/1801.07736.pdf), 2018

**Problem Statement:** Portions of a body of text are deleted, to infill while indistinguishable from original set

**Experiment:** 
1. Train a language model using MLE. Use pretrained model weights for seq2seq encoder/decoder modules. 
2. Pretrain seq2seq models on in-filling task using MLE, using attention parameters. 
3. Select model producing lowest validation perplexity. 

**Result**: Training where contiguous blocks of words were masked produced better samples. Use of attention was important for in-filled words to be sufficiently conditioned on the input context

**Code:** https://github.com/tensorflow/models/tree/ master/research/maskgan

### [Neural. text generation: past present beyond](https://arxiv.org/pdf/1803.07133.pdf), 2018

**Problem Statement:** Empircal study of typical neural text generation models with image captioning tasks: SeqGAN, MaliGAN, RankGAN, LeakGAN, MaskGAN, Text GAN, MLE

**Dataset:** Conditional and unconditional samples generated on Penn Treebank and IMDB datasets at word level

**Experiment:** Initialise GAN model parameters in a standard Gaussian distribution, pretrain each models' generator and discriminator using the MLE training of 80 epochs

**Models:** SeqGAN, MaliGAN, RankGAN, LeakGAN, MaskGAN, Text GAN, MLE

**Results:** Measure: BLEU score. SeqGAN works best, MaliGAN works best on long texts generation

### [Survey of the state of the art natural language generation: core tasks, applications and evaluation](https://arxiv.org/pdf/1703.09902.pdf), 2018

**Notes:** BLEU score is biased towards length of texts. With short texts, n-gram based measures do not perform well 

### [Lagging Inference Networks and Posterior Collapse in VAEs](https://arxiv.org/pdf/1901.05534.pdf), 2019

### [NAACL Tutorial on GANs](	https://sites.cs.ucsb.edu/~william/papers/AdvNLP-NAACL2019.pdf), 2019

### [Language models as knowledge bases?](https://arxiv.org/pdf/1909.01066.pdf), 2019

**Code:** https://github.com/facebookresearch/LAMA

